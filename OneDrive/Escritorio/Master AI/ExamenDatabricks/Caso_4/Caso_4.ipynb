{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6faa8f82-8459-4b9d-b8be-b985c9f761af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Limpieza de datos con PySpark: Data Science Job Posting on Glassdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffad86de-2091-43f9-97e4-c94456fee761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Los [datos](https://tajamar365.sharepoint.com/:x:/s/3405-MasterIA2024-2025/ETYTQ0c-i6FLjM8rZ4iT1cgB6ipFAkainM-4V9M8DXsBiA?e=PeMtvh) fueron extraídos (scrapeados) del sitio web de Glassdoor y recoge los salarios de distintos puestos relacionados a Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd64de3-16db-4ce1-89ee-026c842262a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Resolver los siguientes requerimientos, para cada operación/moficación imprima como van quedadndo los cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22e55e35-5fe8-4a9a-a14a-d8d15dbe45b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Cargar los datos y mostrar el esquema o la informacion de las columnas y el tip de dato de cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e5b0f19-638f-41f7-9566-11cb7c449f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "               .option(\"delimiter\", \";\") \\\n",
    "               .option(\"multiline\", \"true\") \\\n",
    "               .option(\"quote\", \"\\\"\") \\\n",
    "               .option(\"escape\", \"\\\"\") \\\n",
    "               .csv(\"/FileStore/Notebook Examen/Caso4/ds_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "433c410b-ccf9-4287-a273-fa062bee87f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a4b636b-5e44-4afa-9d44-0666b991cec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Eliminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87090a87-8143-4963-b470-c06b27656e67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar las funciones necesarias\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Agrupar por todas las columnas y contar las ocurrencias\n",
    "df_duplicates = df.groupBy(df.columns).agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Filtrar solo los duplicados\n",
    "df_duplicates = df_duplicates.filter(col(\"count\") > 1)\n",
    "\n",
    "# Mostrar los duplicados\n",
    "df_duplicates.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b11dbb-14e7-4635-ad53-edf751ccd3a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Eliminar los registros duplicados basados en todas las columnas, aunque en este caso no se encuentra ninguno\n",
    "df_clean = df.dropDuplicates()\n",
    "\n",
    "# Mostrar el DataFrame limpio\n",
    "df_clean.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9b97746-8b29-4550-9e1b-be2876c2a0a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Decidir que hacer con los datos faltantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac43d03f-6147-4a26-88ac-881c2dcd40e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar las funciones necesarias\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Verificar si hay nulos en cada columna\n",
    "null_counts = df_clean.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "# Mostrar el conteo de nulos por columna\n",
    "null_counts.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a669b54-49b2-45f6-8b00-4f4fed475e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "# Filtrar las filas que tienen al menos un valor nulo\n",
    "rows_with_nulls = df_clean.filter(reduce(lambda x, y: x | y, (col(c).isNull() for c in df.columns)))\n",
    "\n",
    "# Mostrar las filas con valores nulos\n",
    "rows_with_nulls.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "147e90f5-1450-46fe-956c-21f52af03dd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Eliminar las filas con cualquier valor nulo \n",
    "df_clean = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaedda51-4a41-41cd-aba7-75f0e5a3c5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Decidir que hacer con los valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d88aca13-ba09-4719-be0c-5b963dbf5e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#hemos realizado este apartado en el punto anterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb3b1ca-24ac-4206-8906-3ecee5d6c5aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. ¿Cuántos registros tiene el csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a33851b-8efd-46ee-9a36-0cf27a98e2bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contar el número de registros en el DataFrame\n",
    "num_registros = df_clean.count()\n",
    "\n",
    "# Mostrar el número de registros\n",
    "print(f\"El número de registros en el CSV es: {num_registros}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e9aac77-6fe5-4a54-b6ae-c152feee149f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Mostrar los valores únicos de `Job title` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d09dd53-e0fb-4bb2-b813-8d91b7285570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar los valores únicos de la columna 'Job Title'\n",
    "unique_job_titles = df.select(\"Job Title\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "unique_job_titles.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df06a700-7dd6-44c8-877c-52819ffb73a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Remover la letra `K` de la columna `Salary Estimate` y multiplicar por 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "789275d8-2c20-496d-a52f-a9de58fdf101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Mostrar los valores originales de 'Salary Estimate'\n",
    "print(\"Valores originales de 'Salary Estimate':\")\n",
    "df_clean.select(\"Salary Estimate\").show(truncate=False)\n",
    "\n",
    "# Remover la letra 'K' y reemplazarla por '000'\n",
    "df_clean_transformed = df_clean.withColumn(\"Salary Estimate\", regexp_replace(col(\"Salary Estimate\"), \"K\", \"000\"))\n",
    "\n",
    "# Mostrar los valores transformados de 'Salary Estimate'\n",
    "print(\"Valores transformados de 'Salary Estimate':\")\n",
    "df_clean_transformed.select(\"Salary Estimate\").show(truncate=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0f2bbcd-5c78-4d10-9118-e105f611b4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Mostrar los valores únicos del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9efb2f84-d885-4d3b-8141-bcafdfc2f16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar los valores únicos de 'Salary Estimate'\n",
    "unique_salary_estimates = df_clean_transformed.select(\"Salary Estimate\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "unique_salary_estimates.display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da5d10ab-fd9e-4d8b-bf7f-f8d25b9ed1af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. Eliminar `(Glassdoor est.)` y `(Employer est.)` del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b66bd24-c5c2-4bd8-b470-5e2ddbb08b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Mostrar los valores originales de 'Salary Estimate'\n",
    "print(\"Valores originales de 'Salary Estimate':\")\n",
    "df_clean_transformed.select(\"Salary Estimate\").show(truncate=False)\n",
    "\n",
    "# Remover '(Glassdoor est.)' y '(Employer est.)'\n",
    "df_final = df_clean_transformed.withColumn(\"Salary Estimate\", \n",
    "    regexp_replace(col(\"Salary Estimate\"), r\"\\s*\\(Glassdoor est.\\)|\\(Employer est.\\)\", \"\"))\n",
    "\n",
    "# Mostrar los valores transformados de 'Salary Estimate'\n",
    "print(\"Valores transformados de 'Salary Estimate':\")\n",
    "df_final.select(\"Salary Estimate\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ff37c4-17a0-4eb4-b170-a131bd9b7844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Mostrar de mayor a menor los valores del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60c41cae-ee5e-4dbd-be91-e870e4236d8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, split\n",
    "\n",
    "# Remover el símbolo '$'\n",
    "df_cleaned_final = df_final.withColumn(\"Salary Estimate\", regexp_replace(col(\"Salary Estimate\"), r\"\\$\", \"\"))\n",
    "\n",
    "# Separar los valores del rango salarial en dos columnas nuevas\n",
    "df_split = df_cleaned_final.withColumn(\"Salary Low\", split(col(\"Salary Estimate\"), \"-\").getItem(0).cast(\"float\"))\n",
    "df_split = df_split.withColumn(\"Salary High\", split(col(\"Salary Estimate\"), \"-\").getItem(1).cast(\"float\"))\n",
    "\n",
    "# Eliminar la columna original 'Salary Estimate'\n",
    "df_final2 = df_split.drop(\"Salary Estimate\")\n",
    "\n",
    "# Mostrar los valores transformados\n",
    "print(\"Valores transformados:\")\n",
    "df_final2.display(truncate=False)\n",
    "\n",
    "# Ordenar y mostrar los valores de 'Salary High' de mayor a menor\n",
    "df_sorted = df_final2.orderBy(col(\"Salary High\").desc())\n",
    "df_sorted.select(\"Salary High\", \"Salary Low\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c20074ca-ff4d-45ba-a548-369bd3bdadc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11. De la columna `Job Description` quitar los saltos de linea `\\n` del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b2c652-bb50-41d7-9376-667cf6b0045a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Mostrar los valores originales de 'Job Description'\n",
    "print(\"Valores originales de 'Job Description':\")\n",
    "df_final2.select(\"Job Description\").display(truncate=False)\n",
    "\n",
    "# Quitar los saltos de línea (\\n) del texto\n",
    "df_cleaned = df_final2.withColumn(\"Job Description\", regexp_replace(col(\"Job Description\"), r\"\\n\", \" \"))\n",
    "\n",
    "# Mostrar los valores transformados de 'Job Description'\n",
    "print(\"Valores transformados de 'Job Description':\")\n",
    "df_cleaned.select(\"Job Description\").display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63b8b421-3881-4bae-95c0-b79a37422e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12. De la columna `Rating` muestre los valores unicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc3cd79-27cf-4549-8d5d-c282268b91dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar los valores únicos de la columna 'Rating'\n",
    "unique_ratings = df_cleaned.select(\"Rating\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "unique_ratings.display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd85fcdd-3514-4ad2-ab52-d1f21b825f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "13. Del campo `Rating` reemplazar los `-1.0` por `0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc3b7f9-2d93-4d20-b658-561fc32bb7d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Reemplazar los valores -1.0 por 0.0 en la columna 'Rating'\n",
    "df_replaced = df_cleaned.withColumn(\"Rating\", when(col(\"Rating\") == -1.0, 0.0).otherwise(col(\"Rating\")))\n",
    "\n",
    "# Mostrar los valores únicos de la columna 'Rating' después de la transformación\n",
    "print(\"Valores únicos de la columna 'Rating' después de la transformación:\")\n",
    "df_replaced.select(\"Rating\").distinct().display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd30cd0d-46a3-4f2f-84f8-4c7e81d7d3ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "14. Mostrar los valores unicos y ordenar los valores del campo `Company Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8753f4d-abb2-44c2-a59a-8fe050f744b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los valores únicos de 'Company Name' y ordenarlos\n",
    "unique_company_names = df_replaced.select(\"Company Name\").distinct().orderBy(\"Company Name\")\n",
    "\n",
    "# Mostrar los valores únicos y ordenados de 'Company Name'\n",
    "unique_company_names.display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd2e2a45-3570-49a6-823c-882f5714a647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "15. Quitar todos los caracteres innecesarios que encuentres en el campo `Company Name`. Por ejemplo los saltos de linea `\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bceddf9-68a2-4a09-8aa2-cf3cb10c62a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Mostrar los valores originales de 'Company Name'\n",
    "print(\"Valores originales de 'Company Name':\")\n",
    "df_replaced.select(\"Company Name\").show(truncate=False)\n",
    "\n",
    "# Quitar los saltos de línea (\\n) y otros caracteres innecesarios\n",
    "df_cleaned = df_replaced.withColumn(\"Company Name\", regexp_replace(col(\"Company Name\"), r\"\\s+\", \" \"))\n",
    "\n",
    "# Mostrar los valores transformados de 'Company Name'\n",
    "print(\"Valores transformados de 'Company Name':\")\n",
    "df_cleaned.select(\"Company Name\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e594ec2-2d3f-429a-8cf4-7a6815567aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "16. En el campo `Location` convertir esa columna en dos: `City` y `State`. Las ciudades que tengas en `Location` asignar a la columna `City`. Lo mismo para `State`. Luego elimine la columna `Location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80464ceb-0a1c-430d-a8f4-5c36b53bf646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Mostrar los valores originales de 'Location'\n",
    "print(\"Valores originales de 'Location':\")\n",
    "df_cleaned.select(\"Location\").show(truncate=False)\n",
    "\n",
    "# Dividir la columna 'Location' en 'City' y 'State'\n",
    "df_split = df_cleaned.withColumn(\"City\", split(col(\"Location\"), \", \").getItem(0))\n",
    "df_split = df_split.withColumn(\"State\", split(col(\"Location\"), \", \").getItem(1))\n",
    "\n",
    "# Eliminar la columna original 'Location'\n",
    "df_final = df_split.drop(\"Location\")\n",
    "\n",
    "# Mostrar los valores transformados\n",
    "print(\"Valores transformados:\")\n",
    "df_final.select(\"City\", \"State\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ed408ad-803a-4209-9dae-57b7418724ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "17. Repetir la misma lógica de la pregunta 16 pero para el campo `Headquarters`. En Headquarters dejar solo la ciudad, mientras que para el estado añadirla a una columna nueva ` Headquarter State`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75cb2f2b-7fca-459c-b2d5-f100058bcd96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Mostrar los valores originales de 'Headquarters'\n",
    "print(\"Valores originales de 'Headquarters':\")\n",
    "df_final.select(\"Headquarters\").show(truncate=False)\n",
    "\n",
    "# Dividir la columna 'Headquarters' en 'Headquarter City' y 'Headquarter State'\n",
    "df_split_hq = df_final.withColumn(\"Headquarter City\", split(col(\"Headquarters\"), \", \").getItem(0))\n",
    "df_split_hq = df_split_hq.withColumn(\"Headquarter State\", split(col(\"Headquarters\"), \", \").getItem(1))\n",
    "\n",
    "# Eliminar la columna original 'Headquarters'\n",
    "df_final_hq = df_split_hq.drop(\"Headquarters\")\n",
    "\n",
    "# Mostrar los valores transformados\n",
    "print(\"Valores transformados:\")\n",
    "df_final_hq.select(\"Headquarter City\", \"Headquarter State\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09c9f37c-dbe5-4f0e-8f9b-7c4b4cfe8ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "18. Muestre los valores únicos del campo `Headquarter State` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcaf0f25-66ad-4b87-8eb5-01c9238003ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los valores únicos de 'Headquarter State'\n",
    "unique_headquarter_states = df_final_hq.select(\"Headquarter State\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "unique_headquarter_states.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7895f10-d9c8-443b-a225-848031030eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "19. Mostrar valores unicos del campo `Size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa214dc-4dcb-464e-a744-faeb36d540c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los valores únicos de 'Size'\n",
    "unique_sizes = df_final_hq.select(\"Size\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "unique_sizes.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8f6a6b8-52b5-4e2e-9850-6dadc97f206c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "20. Quitar 'employee' de los registros del campo `Size`. Elimine tambien otros caracteres basura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a66f4f-5f98-481f-84e2-6f917578fae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Mostrar los valores originales de 'Size'\n",
    "print(\"Valores originales de 'Size':\")\n",
    "df_final_hq.select(\"Size\").show(truncate=False)\n",
    "\n",
    "# Quitar la palabra 'employees' y otros caracteres basura\n",
    "df_cleaned = df_final_hq.withColumn(\"Size\", regexp_replace(col(\"Size\"), r\"employees?|[*+]\", \"\").cast(\"string\"))\n",
    "\n",
    "# Mostrar los valores transformados de 'Size'\n",
    "print(\"Valores transformados de 'Size':\")\n",
    "df_cleaned.select(\"Size\").show(truncate=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36dfe0a0-9a25-40bc-8282-cba498d2badd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "21. Reemplazar la palabra 'to' por '-' en todos los registros del campo `Size`. Reemplazar tambien '-1' por 'Unknown'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf40c4dd-5216-4259-9db6-5d92c3e0b38e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Mostrar los valores originales de 'Size'\n",
    "print(\"Valores originales de 'Size':\")\n",
    "df_cleaned.select(\"Size\").show(truncate=False)\n",
    "\n",
    "# Reemplazar 'to' por '-' y '-1' por 'Unknown'\n",
    "df_transformed = df_cleaned.withColumn(\"Size\", regexp_replace(col(\"Size\"), r\"\\bto\\b\", \"-\"))\n",
    "df_transformed = df_transformed.withColumn(\"Size\", regexp_replace(col(\"Size\"), r\"-1\", \"Unknown\"))\n",
    "\n",
    "# Mostrar los valores transformados de 'Size'\n",
    "print(\"Valores transformados de 'Size':\")\n",
    "df_transformed.select(\"Size\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f598578e-b86d-4de7-a298-709ec19648ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "22. Mostrar el tipo de dato del campo `Type of ownership` y sus registros unicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b9415f0-cf8b-4819-83b3-98915f19c735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna 'Type of ownership'\n",
    "print(\"Tipo de dato de 'Type of ownership':\")\n",
    "df_transformed.select(\"Type of ownership\").printSchema()\n",
    "\n",
    "# Mostrar los registros únicos de la columna 'Type of ownership'\n",
    "unique_ownership_types = df_transformed.select(\"Type of ownership\").distinct()\n",
    "print(\"Registros únicos de 'Type of ownership':\")\n",
    "unique_ownership_types.display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f42ef72-f068-4b46-b1d3-6a5d59290322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "23. Cambiar '-1' por 'Unknown' en todos los registros del campo `Type of ownership`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb48b39d-0e6d-4fe9-a495-a16eceb2e79d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Reemplazar los valores -1 por 'Unknown' en la columna 'Type of ownership'\n",
    "df_replaced = df_transformed.withColumn(\"Type of ownership\", when(col(\"Type of ownership\") == \"-1\", \"Unknown\").otherwise(col(\"Type of ownership\")))\n",
    "\n",
    "# Mostrar los registros únicos de 'Type of ownership' después de la transformación\n",
    "print(\"Registros únicos de 'Type of ownership' después de la transformación:\")\n",
    "unique_ownership_types_after = df_replaced.select(\"Type of ownership\").distinct()\n",
    "unique_ownership_types_after.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9f9087-9cce-4213-b99b-f573757dbd97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "24. Cambiar:  \n",
    "-  `Company - Public` por `Public Company`  \n",
    "-  `Company - Private` por `Private Company`  \n",
    "-  `Private Practice / Firm` por `Private Company`  \n",
    "-  `Subsidiary or Business Segment` por `Business`  \n",
    "-  `College / University` por `Education`  \n",
    "En todos los registros del campo `Type of ownership`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b7fbba-2db6-4855-abd8-9f5c15113230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Mostrar los valores originales de 'Type of ownership'\n",
    "print(\"Valores originales de 'Type of ownership':\")\n",
    "df_replaced.select(\"Type of ownership\").distinct().show(truncate=False)\n",
    "\n",
    "# Realizar las transformaciones en 'Type of ownership'\n",
    "df_transformed = df_replaced.withColumn(\"Type of ownership\",\n",
    "    when(col(\"Type of ownership\") == \"Company - Public\", \"Public Company\")\n",
    "    .when(col(\"Type of ownership\") == \"Company - Private\", \"Private Company\")\n",
    "    .when(col(\"Type of ownership\") == \"Private Practice / Firm\", \"Private Company\")\n",
    "    .when(col(\"Type of ownership\") == \"Subsidiary or Business Segment\", \"Business\")\n",
    "    .when(col(\"Type of ownership\") == \"College / University\", \"Education\")\n",
    "    .otherwise(col(\"Type of ownership\"))\n",
    ")\n",
    "\n",
    "# Mostrar los valores transformados de 'Type of ownership'\n",
    "print(\"Valores transformados de 'Type of ownership':\")\n",
    "df_transformed.select(\"Type of ownership\").distinct().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed329cf5-1daa-4dd9-9b16-d2ab23bbf486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "25. Mostrar el tipo de dato y los valores unicos del campo `Industry`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f96cfa-d3d4-4b34-ab98-24ab2ed99622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna 'Industry'\n",
    "print(\"Tipo de dato de 'Industry':\")\n",
    "df_transformed.select(\"Industry\").printSchema()\n",
    "\n",
    "# Mostrar los registros únicos de la columna 'Industry'\n",
    "unique_industries = df_transformed.select(\"Industry\").distinct()\n",
    "print(\"Registros únicos de 'Industry':\")\n",
    "unique_industries.display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c06a12fb-1a33-4cfa-aadb-c01e00a9c4ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "26. En el mismo campo de `Industry` reemplazar '-1' por 'Not Available' y '&' por 'and'.  Vuelva a imprimir los valores unicos en orden alfabético."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d0bd2bf-937c-4a15-87d5-81dd67ec92ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Mostrar el tipo de dato y los valores únicos del campo Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cced18d-78b7-4899-a962-d5b01a2d4c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna 'Industry'\n",
    "print(\"Tipo de dato de 'Industry':\")\n",
    "df_transformed.select(\"Industry\").printSchema()\n",
    "\n",
    "# Mostrar los registros únicos de la columna 'Industry' antes de la transformación\n",
    "print(\"Registros únicos de 'Industry' antes de la transformación:\")\n",
    "unique_industries_before = df_transformed.select(\"Industry\").distinct()\n",
    "unique_industries_before.display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56ed26ff-a93a-4417-b02b-f3cab6852482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Realizar las transformaciones en el campo Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e3632af-db36-4967-b4f6-d6cff987403d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "# Reemplazar '-1' por 'Not Available' y '&' por 'and'\n",
    "df_transformed1 = df_transformed.withColumn(\"Industry\", regexp_replace(col(\"Industry\"), r\"-1\", \"Not Available\"))\n",
    "df_transformed1 = df_transformed1.withColumn(\"Industry\", regexp_replace(col(\"Industry\"), r\"&\", \"and\"))\n",
    "\n",
    "# Mostrar los registros únicos de la columna 'Industry' después de la transformación\n",
    "print(\"Registros únicos de 'Industry' después de la transformación y ordenados alfabéticamente:\")\n",
    "unique_industries_after = df_transformed1.select(\"Industry\").distinct().orderBy(\"Industry\")\n",
    "unique_industries_after.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2490bb6-1f88-4410-a0c4-75826c48b70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "27. Para el campo `Sector`, muestre el tipo de dato y los valores únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1544e46b-eb71-43de-a92f-952d985617be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna 'Sector'\n",
    "print(\"Tipo de dato de 'Sector':\")\n",
    "df_transformed1.select(\"Sector\").printSchema()\n",
    "\n",
    "# Mostrar los registros únicos de la columna 'Sector'\n",
    "unique_sectors = df_transformed1.select(\"Sector\").distinct()\n",
    "print(\"Registros únicos de 'Sector':\")\n",
    "unique_sectors.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d92b2b-c4be-4aca-b734-3ef8d80b62cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "28. Aplica la misma lógica de la pregunta 26 pero sobre el campo `Sector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e583bbd5-d3e9-49e5-8919-d2fbf6eda791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "# Reemplazar '-1' por 'Not Available' y '&' por 'and'\n",
    "df_transformed2 = df_transformed1.withColumn(\"Sector\", regexp_replace(col(\"Sector\"), r\"-1\", \"Not Available\"))\n",
    "df_transformed2 = df_transformed2.withColumn(\"Sector\", regexp_replace(col(\"Sector\"), r\"&\", \"and\"))\n",
    "\n",
    "# Mostrar los registros únicos de la columna 'Sector' después de la transformación\n",
    "print(\"Registros únicos de 'Sector' después de la transformación y ordenados alfabéticamente:\")\n",
    "unique_sectors_after = df_transformed2.select(\"Sector\").distinct().orderBy(\"Sector\")\n",
    "unique_sectors_after.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a3770cb-694e-439f-8038-d28da4e3ac0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "29. Para el campo `Revenue`, muestre el tipo de dato y los valores únicos en orden ascedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4f0fb04-136e-48c8-8e35-4ab3366cb343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna 'Revenue'\n",
    "print(\"Tipo de dato de 'Revenue':\")\n",
    "df_transformed2.select(\"Revenue\").printSchema()\n",
    "\n",
    "# Mostrar los registros únicos de la columna 'Revenue' en orden ascendente\n",
    "unique_revenues = df_transformed2.select(\"Revenue\").distinct().orderBy(\"Revenue\")\n",
    "print(\"Registros únicos de 'Revenue' en orden ascendente:\")\n",
    "unique_revenues.display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cb56eb9-3684-49cd-b20f-fd368d23b499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "30. En el campo `Revenue`, cambiar:  \n",
    "-  `-1` por `N/A`  \n",
    "-  `Unknown / Non-Applicable` por `N/A`  \n",
    "-  `Less than $1 million (USD)` por `Less than 1`\n",
    "-  Quitar `$` y `(USD)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40ccafbe-7d6f-4068-bbf5-8e2c34780b09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "# Realizar las transformaciones en 'Revenue'\n",
    "df_transformed3 = df_transformed2.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"-1\", \"N/A\"))\n",
    "df_transformed3 = df_transformed3.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"Unknown / Non-Applicable\", \"N/A\"))\n",
    "df_transformed3 = df_transformed3.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"Less than \\$1 million \\(USD\\)\", \"Less than 1\"))\n",
    "df_transformed3 = df_transformed3.withColumn(\"Revenue\", regexp_replace(col(\"Revenue\"), r\"[\\$\\(USD\\)]\", \"\"))\n",
    "\n",
    "# Mostrar los registros únicos de la columna 'Revenue' después de la transformación\n",
    "print(\"Registros únicos de 'Revenue' después de la transformación y ordenados alfabéticamente:\")\n",
    "unique_revenues_after = df_transformed3.select(\"Revenue\").distinct().orderBy(\"Revenue\")\n",
    "unique_revenues_after.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d89472f2-855f-4edc-bc47-d2654069bf38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "31. Borrar el campo `Competitors`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15549635-7d09-431f-8b0f-b1e31a313727",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Mostrar el esquema y valores únicos antes de eliminar la columna Competitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f908655-938d-49ae-a74f-1eb2579fd8c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el esquema del DataFrame antes de eliminar la columna 'Competitors'\n",
    "print(\"Esquema del DataFrame antes de eliminar la columna 'Competitors':\")\n",
    "df_transformed3.printSchema()\n",
    "\n",
    "# Mostrar algunos registros para verificar el contenido\n",
    "print(\"Valores del DataFrame antes de eliminar la columna 'Competitors':\")\n",
    "df_transformed3.display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "096e3a94-4f36-49cc-906c-0aea17915415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Eliminar la columna Competitors y mostrar el esquema y valores únicos después"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8300a3c4-ca8a-42ff-b047-8889091d6243",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Eliminar la columna 'Competitors'\n",
    "df_dropped = df_transformed3.drop(\"Competitors\")\n",
    "\n",
    "# Mostrar el esquema del DataFrame después de eliminar la columna 'Competitors'\n",
    "print(\"Esquema del DataFrame después de eliminar la columna 'Competitors':\")\n",
    "df_dropped.printSchema()\n",
    "\n",
    "# Mostrar algunos registros para verificar el contenido\n",
    "print(\"Valores del DataFrame después de eliminar la columna 'Competitors':\")\n",
    "df_dropped.display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1d3b8e8-a020-4a19-9d85-fb3f635ed316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "32. Crear tres columnas: `min_salary` (salario mínimo), `max_salary` (salario maximo) y `avg_salary` (salario promedio) a partir de los datos del campo `Salary Estimate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39f61e49-ed6d-4b88-b166-ab3963cfcbd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col, lit\n",
    "# Mostrar el esquema del DataFrame\n",
    "#La creación de las tabklas salario minimo y maximo ya han sido creadas antes\n",
    "df_dropped.printSchema()\n",
    "\n",
    "# Calcular el salario promedio\n",
    "df = df_dropped.withColumn(\"avg_salary\", ((col(\"Salary Low\") + col(\"Salary High\")) / 2).cast(\"float\"))\n",
    "\n",
    "# Mostrar los nuevos valores del DataFrame\n",
    "print(\"Valores del DataFrame después de agregar 'Salary Low', 'Salary High' y 'avg_salary':\")\n",
    "df.select(\"Salary Low\", \"Salary High\", \"avg_salary\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "359b377b-30ee-4c05-b7cd-f0f217a4d9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "33. Mostrar los valores unicos del campo `Founded` y el tipo de dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e31632-9fc4-4f74-b657-7b5f4f289b30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna 'Founded'\n",
    "print(\"Tipo de dato de 'Founded':\")\n",
    "df.select(\"Founded\").printSchema()\n",
    "# Mostrar los registros únicos de la columna 'Founded'\n",
    "unique_founded = df.select(\"Founded\").distinct().orderBy(\"Founded\")\n",
    "print(\"Registros únicos de 'Founded':\")\n",
    "unique_founded.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd2dd8d-9e02-46e5-b441-0b93d2775f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "34. Reemplazar '-1' por '2024' en todos los registros del campo `Founded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccc7adc-a063-4759-ac3d-a0d10aeb0346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Reemplazar los valores -1 por 2024 en la columna 'Founded'\n",
    "df_transformed = df.withColumn(\"Founded\", when(col(\"Founded\") == -1, 2024).otherwise(col(\"Founded\")))\n",
    "\n",
    "# Mostrar los registros únicos de la columna 'Founded' después de la transformación\n",
    "print(\"Registros únicos de 'Founded' después de la transformación:\")\n",
    "unique_founded_after = df_transformed.select(\"Founded\").distinct().orderBy(\"Founded\")\n",
    "unique_founded_after.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e823336d-0258-4af5-8d13-c9a358b9295f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "35. Crear una nueva columna o campo que se llame `company_age` con los datos que se deducen del campo `Founded`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "271849ca-a79b-4fa0-aa34-81b127d06a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Mostrar el tipo de dato y los valores únicos del campo Founded antes de la transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a14ab37-0539-490d-8339-e84985487cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna 'Founded'\n",
    "print(\"Tipo de dato de 'Founded':\")\n",
    "df_transformed.select(\"Founded\").printSchema()\n",
    "\n",
    "# Mostrar los registros únicos de la columna 'Founded' antes de la transformación\n",
    "print(\"Registros únicos de 'Founded' antes de la transformación:\")\n",
    "unique_founded_before = df_transformed.select(\"Founded\").distinct().orderBy(\"Founded\")\n",
    "unique_founded_before.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6ec3a31-bd9b-434e-a6cb-c2ef64109610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Crear la nueva columna company_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44aa58f7-bd35-40e8-9a69-fdd314ef158a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Año actual\n",
    "current_year = 2024\n",
    "\n",
    "# Calcular la edad de la compañía\n",
    "df = df_transformed.withColumn(\"company_age\", current_year - col(\"Founded\"))\n",
    "\n",
    "# Mostrar los nuevos valores del DataFrame\n",
    "print(\"Valores del DataFrame después de agregar 'company_age':\")\n",
    "df.select(\"Founded\", \"company_age\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49a8f7ed-434c-4ad1-ba31-eeecd65c117c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "36. Crear una columna o campo que se llame: `Job Type` y en cada registro debe ir Senior, Junior o NA según los datos del campo `Job Title`.  \n",
    "- Cambiar 'sr' o 'senior' o 'lead' o 'principal' por `Senior` en el campo `Job Type`. No olvidar las mayúsculas.\n",
    "- Cambiar 'jr' o 'jr.' o cualquier otra variante por `Junior`.  \n",
    "- En cualquier otro caso distinto a los anteriores añadir NA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cfb50e6-1b16-4431-bee0-66b664274278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Mostrar el esquema y algunos registros del campo Job Title antes de la transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ae2e55-ea34-437f-b2aa-b47dcef9bfc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el esquema del DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Mostrar algunos registros del campo 'Job Title'\n",
    "print(\"Valores del campo 'Job Title' antes de la transformación:\")\n",
    "df.select(\"Job Title\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f45ac827-6c88-4e4f-84e7-39854a8e0f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Crear la nueva columna Job Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a94b2eb-88f4-4602-91fb-256be1d87a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lower\n",
    "\n",
    "# Crear la columna 'Job Type' basada en las condiciones especificadas\n",
    "df = df.withColumn(\"Job Type\",\n",
    "    when(lower(col(\"Job Title\")).rlike(\"sr|senior|lead|principal\"), \"Senior\")\n",
    "    .when(lower(col(\"Job Title\")).rlike(\"jr|jr\\\\.\"), \"Junior\")\n",
    "    .otherwise(\"NA\")\n",
    ")\n",
    "\n",
    "# Mostrar los nuevos valores del DataFrame\n",
    "print(\"Valores del DataFrame después de agregar 'Job Type':\")\n",
    "df.select(\"Job Title\", \"Job Type\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c70227b-c16c-4919-9aa2-9bf9514b76dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "37. Muestra los registros únicos del campo `Job Type`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd2e8e6b-b530-4ee0-a955-1b85154b3346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar los registros únicos de la columna 'Job Type'\n",
    "unique_job_types = df.select(\"Job Type\").distinct()\n",
    "print(\"Registros únicos de 'Job Type':\")\n",
    "unique_job_types.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cd4dbad-048d-461f-bf2a-f54c557d16f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "38. Partiendo del campo `Job Description` se extraer todas o las principales skills solicitadas por las empresas, por ejemplo: Python, Spark , Big Data. Cada Skill debe ir en una nueva columna de tipo Binaria ( 0 , 1) o Booleana (True,  False) de modo que cada skill va ser una nueva columna y si esa skill es solicitada por la empresa colocar 1 sino colocar 0. Por ejemplo:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f18e84c-422c-4679-9db3-02ded4f42d2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Definir las habilidades a extraer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c79abc-b47f-408c-9153-4e0904856b5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definir las principales habilidades a extraer\n",
    "skills = [\"Python\", \"Spark\", \"Big Data\", \"SQL\", \"Machine Learning\", \"AWS\", \"Hadoop\", \"Java\", \"Tableau\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aec2c05c-e5e1-44f4-ad81-07af1dac6cee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Crear nuevas columnas binarias para cada habilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27384649-1127-4e8c-b6d7-c6dbed687835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lower\n",
    "\n",
    "# Crear nuevas columnas para cada habilidad\n",
    "for skill in skills:\n",
    "    df = df.withColumn(skill, when(lower(col(\"Job Description\")).contains(skill.lower()), 1).otherwise(0))\n",
    "\n",
    "# Mostrar los nuevos valores del DataFrame con las columnas de habilidades\n",
    "df.select(\"Job Description\", *skills).display(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80e86af1-e86e-48db-b9d8-832cee782b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Por ejemplo:  \n",
    "| Job Title         | Salary Estimate | Job Description                                 | Rating | Company Name       | Size       | Founded | Type of ownership         | Industry                       | Sector                         | Same State      | company_age | Python | Excel |\n",
    "|--------------------|-----------------|-------------------------------------------------|--------|--------------------|------------|---------|---------------------------|--------------------------------|--------------------------------|----------------|-------------|--------|-------|\n",
    "| Sr Data Scientist | 137000-171000   | Description The Senior Data Scientist is resp... | 3.1    | Healthfirst        | 1001-5000  | 1993    | Nonprofit Organization    | Insurance Carriers            | Insurance Carriers            | Same State      | 31          | 0      | 0     |\n",
    "| Data Scientist    | 137000-171000   | Secure our Nation, Ignite your Future Join th... | 4.2    | ManTech            | 5001-10000 | 1968    | Public Company            | Research and Development      | Research and Development      | Same State      | 56          | 0      | 0     |\n",
    "| Data Scientist    | 137000-171000   | Overview Analysis Group is one of the larges... | 3.8    | Analysis Group      | 1001-5000  | 1981    | Private Company           | Consulting                    | Consulting                    | Same State      | 43          | 1      | 1     |\n",
    "| Data Scientist    | 137000-171000   | JOB DESCRIPTION: Do you have a passion for Da... | 3.5    | INFICON            | 501-1000   | 2000    | Public Company            | Electrical and Electronic Manufacturing | Electrical and Electronic Manufacturing | Different State | 24          | 1      | 1     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5609a545-1510-4707-a0bd-a937e37675a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "39. Exportar dataset final a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a7cdff-6999-48dd-9184-80bd5e5fcf68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41fe24a6-5fc3-4ec4-9b76-56fd2b3e36ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "40. Extraer todos los insights posibles que sean de valor o utilidad. Cree nuevas columnas, agrupar,  filtrar hacer varios plots que muestren dichos insights que sean de utilidad para una empresa o para un usuario. Elabore conclusiones con los insights encontrados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47336ad8-9ded-4bf1-bb31-77bee1ff1d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calcular la media del salario promedio por tipo de trabajo\n",
    "avg_salary_by_job_type = df.groupBy(\"Job Type\").avg(\"avg_salary\").orderBy(\"avg(avg_salary)\", ascending=False)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Media del salario promedio por tipo de trabajo:\")\n",
    "avg_salary_by_job_type.show(truncate=False)\n",
    "\n",
    "# Convertir a pandas para visualizar\n",
    "avg_salary_by_job_type_pd = avg_salary_by_job_type.toPandas()\n",
    "\n",
    "# Graficar la media del salario promedio por tipo de trabajo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(avg_salary_by_job_type_pd[\"Job Type\"], avg_salary_by_job_type_pd[\"avg(avg_salary)\"], color=['blue', 'green', 'red'])\n",
    "plt.xlabel(\"Job Type\")\n",
    "plt.ylabel(\"Average Salary\")\n",
    "plt.title(\"Average Salary by Job Type\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4d8898f-3b1a-4cbf-b8d5-a65a4d8aa8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcular la cantidad de trabajos que requieren cada habilidad\n",
    "skills_demand = df.select(skills).groupBy().sum()\n",
    "\n",
    "# Convertir a pandas para visualizar\n",
    "skills_demand_pd = skills_demand.toPandas().transpose()\n",
    "skills_demand_pd.columns = [\"Count\"]\n",
    "skills_demand_pd = skills_demand_pd.sort_values(by=\"Count\", ascending=False)\n",
    "\n",
    "# Graficar la demanda de habilidades\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(skills_demand_pd.index, skills_demand_pd[\"Count\"], color='purple')\n",
    "plt.xlabel(\"Skills\")\n",
    "plt.ylabel(\"Number of Job Listings\")\n",
    "plt.title(\"Demand for Different Skills\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f41466c-497a-443c-b906-3fc64a6f9b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calcular la cantidad de compañías en diferentes rangos de edad\n",
    "age_bins = [0, 5, 10, 20, 50, 100, 200, 300]\n",
    "df = df.withColumn(\"age_bin\", when(col(\"company_age\").between(0, 5), \"0-5 years\")\n",
    "                                 .when(col(\"company_age\").between(6, 10), \"6-10 years\")\n",
    "                                 .when(col(\"company_age\").between(11, 20), \"11-20 years\")\n",
    "                                 .when(col(\"company_age\").between(21, 50), \"21-50 years\")\n",
    "                                 .when(col(\"company_age\").between(51, 100), \"51-100 years\")\n",
    "                                 .when(col(\"company_age\").between(101, 200), \"101-200 years\")\n",
    "                                 .otherwise(\"200+ years\"))\n",
    "\n",
    "# Calcular la cantidad de compañías por rango de edad\n",
    "company_age_distribution = df.groupBy(\"age_bin\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Distribución de la edad de las compañías:\")\n",
    "company_age_distribution.show(truncate=False)\n",
    "\n",
    "# Convertir a pandas para visualizar\n",
    "company_age_distribution_pd = company_age_distribution.toPandas()\n",
    "\n",
    "# Graficar la distribución de la edad de las compañías\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(company_age_distribution_pd[\"age_bin\"], company_age_distribution_pd[\"count\"], color='orange')\n",
    "plt.xlabel(\"Company Age\")\n",
    "plt.ylabel(\"Number of Companies\")\n",
    "plt.title(\"Distribution of Company Age\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a82a7bb-b62d-4e10-bd61-01de4a56f510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna 'Industry' y algunos registros del campo 'avg_salary'\n",
    "df.select(\"Industry\", \"avg_salary\").show(truncate=False)\n",
    "# Calcular la media del salario promedio por industria\n",
    "avg_salary_by_industry = df.groupBy(\"Industry\").avg(\"avg_salary\").orderBy(\"avg(avg_salary)\", ascending=False)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Media del salario promedio por industria:\")\n",
    "avg_salary_by_industry.show(truncate=False)\n",
    "\n",
    "# Convertir a pandas para visualizar\n",
    "avg_salary_by_industry_pd = avg_salary_by_industry.toPandas()\n",
    "\n",
    "# Graficar la media del salario promedio por industria\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.barh(avg_salary_by_industry_pd[\"Industry\"], avg_salary_by_industry_pd[\"avg(avg_salary)\"], color='green')\n",
    "plt.xlabel(\"Average Salary\")\n",
    "plt.ylabel(\"Industry\")\n",
    "plt.title(\"Average Salary by Industry\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1f97ae5-bff6-4abe-b7df-f39d12514e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna 'Industry' y algunos registros del campo 'avg_salary'\n",
    "df.select(\"Industry\", \"avg_salary\").show(truncate=False)\n",
    "# Calcular la media del salario promedio por industria\n",
    "avg_salary_by_industry = df.groupBy(\"Industry\").avg(\"avg_salary\").orderBy(\"avg(avg_salary)\", ascending=False)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Media del salario promedio por industria:\")\n",
    "avg_salary_by_industry.show(truncate=False)\n",
    "\n",
    "# Convertir a pandas para visualizar\n",
    "avg_salary_by_industry_pd = avg_salary_by_industry.toPandas()\n",
    "\n",
    "# Graficar la media del salario promedio por industria\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.barh(avg_salary_by_industry_pd[\"Industry\"], avg_salary_by_industry_pd[\"avg(avg_salary)\"], color='green')\n",
    "plt.xlabel(\"Average Salary\")\n",
    "plt.ylabel(\"Industry\")\n",
    "plt.title(\"Average Salary by Industry\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a557f4d7-7adc-4399-8b31-436d6367b36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el esquema del DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Mostrar algunos registros de los campos 'Size' y 'avg_salary'\n",
    "print(\"Valores de los campos 'Size' y 'avg_salary' antes del análisis:\")\n",
    "df.select(\"Size\", \"avg_salary\").show(truncate=False)\n",
    "\n",
    "# Calcular la media del salario promedio por tamaño de la empresa\n",
    "avg_salary_by_size = df.groupBy(\"Size\").avg(\"avg_salary\").orderBy(\"avg(avg_salary)\", ascending=False)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Media del salario promedio por tamaño de la empresa:\")\n",
    "avg_salary_by_size.show(truncate=False)\n",
    "\n",
    "# Convertir a pandas para visualizar\n",
    "avg_salary_by_size_pd = avg_salary_by_size.toPandas()\n",
    "\n",
    "# Graficar la media del salario promedio por tamaño de la empresa\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(avg_salary_by_size_pd[\"Size\"], avg_salary_by_size_pd[\"avg(avg_salary)\"], color='blue')\n",
    "plt.xlabel(\"Company Size\")\n",
    "plt.ylabel(\"Average Salary\")\n",
    "plt.title(\"Average Salary by Company Size\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbad3a4c-fa55-4abb-82fe-99288f4c7026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna 'Industry' y algunos registros del campo 'avg_salary'\n",
    "df.select(\"Industry\", \"avg_salary\").show(truncate=False)\n",
    "# Calcular la media del salario promedio por industria\n",
    "avg_salary_by_industry = df.groupBy(\"Industry\").avg(\"avg_salary\").orderBy(\"avg(avg_salary)\", ascending=False)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Media del salario promedio por industria:\")\n",
    "avg_salary_by_industry.show(truncate=False)\n",
    "\n",
    "# Convertir a pandas para visualizar\n",
    "avg_salary_by_industry_pd = avg_salary_by_industry.toPandas()\n",
    "\n",
    "# Graficar la media del salario promedio por industria\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.barh(avg_salary_by_industry_pd[\"Industry\"], avg_salary_by_industry_pd[\"avg(avg_salary)\"], color='green')\n",
    "plt.xlabel(\"Average Salary\")\n",
    "plt.ylabel(\"Industry\")\n",
    "plt.title(\"Average Salary by Industry\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Caso_4",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
